{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working, first 650 epochs lr = 1e-4, 650-1450 lr = 5e-5\n",
    "# training took 24h on a P100 on kaggle, the code was not that optimized and a GAN can't be compiled.\n",
    "# had trouble with model colapse and had to lower the batch size and keep precision high\n",
    "# using lighting AI resulted in model collapse every time, I had to revert to pure pytorch for this one.\n",
    "# used the WGAN architecure\n",
    "# I am not proud of this model\n",
    "\n",
    "\n",
    "# next time I would consider an lr scheduler and a lot of other changes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n",
      "number of GPUs: 1\n",
      "name of the GPU: NVIDIA GeForce RTX 3080\n",
      "current device: 0\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from natsort import natsorted\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import wandb\n",
    "\n",
    "import datetime\n",
    "\n",
    "# cuda is available\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda is available')\n",
    "    # get the number of GPUs\n",
    "    print('number of GPUs:', torch.cuda.device_count())\n",
    "    # get the name of the GPU\n",
    "    print('name of the GPU:', torch.cuda.get_device_name(0))\n",
    "    # get the current device\n",
    "    print('current device:', torch.cuda.current_device())\n",
    "\n",
    "    cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfilip-szczepanski\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fil/code/JANGAN/wandb/run-20250109_223427-vs6qt9bk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/filip-szczepanski/JANGAN4/runs/vs6qt9bk' target=\"_blank\">hopeful-star-7</a></strong> to <a href='https://wandb.ai/filip-szczepanski/JANGAN4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/filip-szczepanski/JANGAN4' target=\"_blank\">https://wandb.ai/filip-szczepanski/JANGAN4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/filip-szczepanski/JANGAN4/runs/vs6qt9bk' target=\"_blank\">https://wandb.ai/filip-szczepanski/JANGAN4/runs/vs6qt9bk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/filip-szczepanski/JANGAN4/runs/vs6qt9bk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fdc62741340>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"47080269e7b1b5a51a89830cb24c495498237e77\"\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"JANGAN4\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.000005,\n",
    "    \"architecture\": \"WGAN\",\n",
    "    \"dataset\": \"FFHQ\",\n",
    "    \"epochs\": 3000,\n",
    "    }\n",
    ")\n",
    "# wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3000\n",
    "batch_size = 2048\n",
    "lr = 0.000005\n",
    "n_cpu = os.cpu_count()\n",
    "latent_dim = 128\n",
    "img_size = 64\n",
    "channels = 3\n",
    "n_critic = 5\n",
    "clip_value = 0.1\n",
    "sample_interval = 400\n",
    "\n",
    "img_shape = (channels, img_size, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.shape[0], *img_shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.shape[0], -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.images = [f for f in os.listdir(img_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.CenterCrop(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "dataset = CustomImageDataset(img_dir=\"archive\", transform=transform)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=n_cpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(generator, discriminator, optimizer_G, optimizer_D, epoch, loss_G, loss_D, save_dir):\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "        'loss_G': loss_G,\n",
    "        'loss_D': loss_D\n",
    "    }\n",
    "    \n",
    "    # Save with timestamp and epoch number\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    path = os.path.join(save_dir, f'checkpoint_epoch_{epoch}_{timestamp}.pth')\n",
    "    torch.save(checkpoint, path)\n",
    "    \n",
    "    return path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path, generator, discriminator, optimizer_G, optimizer_D):\n",
    "    checkpoint = torch.load(path)\n",
    "    \n",
    "    # Load model states\n",
    "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "    \n",
    "    # Load optimizer states, uncomment if using a lr scheduler\n",
    "\n",
    "    # optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "    # optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "    \n",
    "    # Return other training info\n",
    "    return checkpoint['epoch'], checkpoint['loss_G'], checkpoint['loss_D']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1997/4149428779.py:56: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789220573/work/torch/csrc/tensor/python_tensor.cpp:78.)\n",
      "  z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/3000] [Batch 0/137] [D loss: 0.053842] [G loss: 0.018178] [Total loss: 0.072020]\n",
      "[Epoch 0/3000] [Batch 5/137] [D loss: -0.119266] [G loss: 0.019328] [Total loss: -0.099938]\n",
      "[Epoch 0/3000] [Batch 10/137] [D loss: -0.268144] [G loss: 0.020459] [Total loss: -0.247685]\n",
      "[Epoch 0/3000] [Batch 15/137] [D loss: -0.389857] [G loss: 0.021716] [Total loss: -0.368141]\n",
      "[Epoch 0/3000] [Batch 20/137] [D loss: -0.534714] [G loss: 0.023180] [Total loss: -0.511535]\n",
      "[Epoch 0/3000] [Batch 25/137] [D loss: -0.702864] [G loss: 0.024720] [Total loss: -0.678144]\n",
      "[Epoch 0/3000] [Batch 30/137] [D loss: -0.866804] [G loss: 0.026452] [Total loss: -0.840351]\n",
      "[Epoch 0/3000] [Batch 35/137] [D loss: -1.020482] [G loss: 0.028324] [Total loss: -0.992158]\n",
      "[Epoch 0/3000] [Batch 40/137] [D loss: -1.266758] [G loss: 0.030409] [Total loss: -1.236349]\n",
      "[Epoch 0/3000] [Batch 45/137] [D loss: -1.373464] [G loss: 0.032645] [Total loss: -1.340819]\n",
      "[Epoch 0/3000] [Batch 50/137] [D loss: -1.619501] [G loss: 0.035021] [Total loss: -1.584480]\n",
      "[Epoch 0/3000] [Batch 55/137] [D loss: -1.794596] [G loss: 0.037612] [Total loss: -1.756984]\n",
      "[Epoch 0/3000] [Batch 60/137] [D loss: -2.010139] [G loss: 0.040427] [Total loss: -1.969712]\n",
      "[Epoch 0/3000] [Batch 65/137] [D loss: -2.311791] [G loss: 0.043272] [Total loss: -2.268519]\n",
      "[Epoch 0/3000] [Batch 70/137] [D loss: -2.455176] [G loss: 0.046549] [Total loss: -2.408627]\n",
      "[Epoch 0/3000] [Batch 75/137] [D loss: -2.661650] [G loss: 0.049739] [Total loss: -2.611911]\n",
      "[Epoch 0/3000] [Batch 80/137] [D loss: -3.017936] [G loss: 0.053373] [Total loss: -2.964563]\n",
      "[Epoch 0/3000] [Batch 85/137] [D loss: -3.358526] [G loss: 0.056947] [Total loss: -3.301579]\n",
      "[Epoch 0/3000] [Batch 90/137] [D loss: -3.797880] [G loss: 0.060601] [Total loss: -3.737278]\n",
      "[Epoch 0/3000] [Batch 95/137] [D loss: -3.795435] [G loss: 0.064572] [Total loss: -3.730863]\n",
      "[Epoch 0/3000] [Batch 100/137] [D loss: -4.242821] [G loss: 0.068491] [Total loss: -4.174330]\n",
      "[Epoch 0/3000] [Batch 105/137] [D loss: -4.491072] [G loss: 0.072370] [Total loss: -4.418702]\n",
      "[Epoch 0/3000] [Batch 110/137] [D loss: -5.110199] [G loss: 0.076361] [Total loss: -5.033839]\n",
      "[Epoch 0/3000] [Batch 115/137] [D loss: -5.173068] [G loss: 0.080304] [Total loss: -5.092763]\n",
      "[Epoch 0/3000] [Batch 120/137] [D loss: -5.215770] [G loss: 0.084386] [Total loss: -5.131383]\n",
      "[Epoch 0/3000] [Batch 125/137] [D loss: -5.841810] [G loss: 0.088201] [Total loss: -5.753610]\n",
      "[Epoch 0/3000] [Batch 130/137] [D loss: -6.307085] [G loss: 0.092106] [Total loss: -6.214979]\n",
      "[Epoch 0/3000] [Batch 135/137] [D loss: -6.767936] [G loss: 0.095545] [Total loss: -6.672391]\n",
      "[Epoch 1/3000] [Batch 0/137] [D loss: -6.637278] [G loss: 0.096036] [Total loss: -6.541241]\n",
      "[Epoch 1/3000] [Batch 5/137] [D loss: -7.146711] [G loss: 0.099296] [Total loss: -7.047415]\n",
      "[Epoch 1/3000] [Batch 10/137] [D loss: -7.269058] [G loss: 0.103296] [Total loss: -7.165763]\n",
      "[Epoch 1/3000] [Batch 15/137] [D loss: -8.004269] [G loss: 0.106442] [Total loss: -7.897827]\n",
      "[Epoch 1/3000] [Batch 20/137] [D loss: -8.207976] [G loss: 0.109441] [Total loss: -8.098535]\n",
      "[Epoch 1/3000] [Batch 25/137] [D loss: -8.512872] [G loss: 0.112324] [Total loss: -8.400547]\n",
      "[Epoch 1/3000] [Batch 30/137] [D loss: -9.358054] [G loss: 0.115068] [Total loss: -9.242986]\n",
      "[Epoch 1/3000] [Batch 35/137] [D loss: -10.244807] [G loss: 0.118368] [Total loss: -10.126439]\n",
      "[Epoch 1/3000] [Batch 40/137] [D loss: -10.045043] [G loss: 0.120224] [Total loss: -9.924819]\n",
      "[Epoch 1/3000] [Batch 45/137] [D loss: -10.083261] [G loss: 0.123045] [Total loss: -9.960216]\n",
      "[Epoch 1/3000] [Batch 50/137] [D loss: -10.895548] [G loss: 0.124253] [Total loss: -10.771294]\n",
      "[Epoch 1/3000] [Batch 55/137] [D loss: -11.917705] [G loss: 0.125226] [Total loss: -11.792479]\n",
      "[Epoch 1/3000] [Batch 60/137] [D loss: -11.540004] [G loss: 0.127008] [Total loss: -11.412996]\n",
      "[Epoch 1/3000] [Batch 65/137] [D loss: -13.370914] [G loss: 0.128330] [Total loss: -13.242583]\n",
      "[Epoch 1/3000] [Batch 70/137] [D loss: -13.548820] [G loss: 0.129178] [Total loss: -13.419643]\n",
      "[Epoch 1/3000] [Batch 75/137] [D loss: -13.507345] [G loss: 0.129432] [Total loss: -13.377913]\n",
      "[Epoch 1/3000] [Batch 80/137] [D loss: -14.320271] [G loss: 0.129136] [Total loss: -14.191135]\n",
      "[Epoch 1/3000] [Batch 85/137] [D loss: -15.010170] [G loss: 0.129320] [Total loss: -14.880850]\n",
      "[Epoch 1/3000] [Batch 90/137] [D loss: -14.816302] [G loss: 0.127276] [Total loss: -14.689027]\n",
      "[Epoch 1/3000] [Batch 95/137] [D loss: -16.090590] [G loss: 0.123043] [Total loss: -15.967546]\n",
      "[Epoch 1/3000] [Batch 100/137] [D loss: -15.918141] [G loss: 0.120159] [Total loss: -15.797982]\n",
      "[Epoch 1/3000] [Batch 105/137] [D loss: -16.843523] [G loss: 0.117010] [Total loss: -16.726513]\n",
      "[Epoch 1/3000] [Batch 110/137] [D loss: -17.483675] [G loss: 0.113501] [Total loss: -17.370174]\n",
      "[Epoch 1/3000] [Batch 115/137] [D loss: -18.869705] [G loss: 0.108057] [Total loss: -18.761649]\n",
      "[Epoch 1/3000] [Batch 120/137] [D loss: -19.000740] [G loss: 0.103156] [Total loss: -18.897584]\n",
      "[Epoch 1/3000] [Batch 125/137] [D loss: -20.286270] [G loss: 0.093248] [Total loss: -20.193022]\n",
      "[Epoch 1/3000] [Batch 130/137] [D loss: -19.832647] [G loss: 0.084438] [Total loss: -19.748210]\n",
      "[Epoch 1/3000] [Batch 135/137] [D loss: -21.656536] [G loss: 0.080733] [Total loss: -21.575804]\n",
      "[Epoch 2/3000] [Batch 0/137] [D loss: -21.445097] [G loss: 0.066137] [Total loss: -21.378960]\n",
      "[Epoch 2/3000] [Batch 5/137] [D loss: -23.200676] [G loss: 0.057806] [Total loss: -23.142870]\n",
      "[Epoch 2/3000] [Batch 10/137] [D loss: -22.926279] [G loss: 0.044306] [Total loss: -22.881973]\n",
      "[Epoch 2/3000] [Batch 15/137] [D loss: -23.344759] [G loss: 0.028027] [Total loss: -23.316732]\n",
      "[Epoch 2/3000] [Batch 20/137] [D loss: -24.614172] [G loss: 0.014952] [Total loss: -24.599220]\n",
      "[Epoch 2/3000] [Batch 25/137] [D loss: -25.537039] [G loss: 0.004357] [Total loss: -25.532682]\n",
      "[Epoch 2/3000] [Batch 30/137] [D loss: -27.193125] [G loss: -0.016817] [Total loss: -27.209942]\n",
      "[Epoch 2/3000] [Batch 35/137] [D loss: -26.558987] [G loss: -0.025547] [Total loss: -26.584534]\n",
      "[Epoch 2/3000] [Batch 40/137] [D loss: -27.791412] [G loss: -0.050151] [Total loss: -27.841564]\n",
      "[Epoch 2/3000] [Batch 45/137] [D loss: -30.119469] [G loss: -0.078608] [Total loss: -30.198076]\n",
      "[Epoch 2/3000] [Batch 50/137] [D loss: -29.160173] [G loss: -0.098387] [Total loss: -29.258561]\n",
      "[Epoch 2/3000] [Batch 55/137] [D loss: -31.093403] [G loss: -0.121302] [Total loss: -31.214705]\n",
      "[Epoch 2/3000] [Batch 60/137] [D loss: -31.307793] [G loss: -0.147226] [Total loss: -31.455018]\n",
      "[Epoch 2/3000] [Batch 65/137] [D loss: -33.050114] [G loss: -0.190939] [Total loss: -33.241052]\n",
      "[Epoch 2/3000] [Batch 70/137] [D loss: -34.454132] [G loss: -0.208568] [Total loss: -34.662700]\n",
      "[Epoch 2/3000] [Batch 75/137] [D loss: -32.829121] [G loss: -0.233922] [Total loss: -33.063042]\n",
      "[Epoch 2/3000] [Batch 80/137] [D loss: -35.585758] [G loss: -0.273717] [Total loss: -35.859475]\n",
      "[Epoch 2/3000] [Batch 85/137] [D loss: -36.272991] [G loss: -0.305291] [Total loss: -36.578282]\n",
      "[Epoch 2/3000] [Batch 90/137] [D loss: -36.966232] [G loss: -0.344755] [Total loss: -37.310987]\n",
      "[Epoch 2/3000] [Batch 95/137] [D loss: -35.540104] [G loss: -0.392407] [Total loss: -35.932511]\n",
      "[Epoch 2/3000] [Batch 100/137] [D loss: -40.228481] [G loss: -0.435520] [Total loss: -40.664002]\n",
      "[Epoch 2/3000] [Batch 105/137] [D loss: -39.568890] [G loss: -0.482721] [Total loss: -40.051611]\n",
      "[Epoch 2/3000] [Batch 110/137] [D loss: -41.034077] [G loss: -0.519209] [Total loss: -41.553285]\n",
      "[Epoch 2/3000] [Batch 115/137] [D loss: -39.757305] [G loss: -0.567192] [Total loss: -40.324497]\n",
      "[Epoch 2/3000] [Batch 120/137] [D loss: -41.035538] [G loss: -0.603902] [Total loss: -41.639440]\n",
      "[Epoch 2/3000] [Batch 125/137] [D loss: -43.110939] [G loss: -0.675656] [Total loss: -43.786595]\n",
      "[Epoch 2/3000] [Batch 130/137] [D loss: -44.123093] [G loss: -0.731008] [Total loss: -44.854101]\n",
      "[Epoch 2/3000] [Batch 135/137] [D loss: -45.227009] [G loss: -0.772263] [Total loss: -45.999271]\n",
      "[Epoch 3/3000] [Batch 0/137] [D loss: -48.888588] [G loss: -0.819284] [Total loss: -49.707872]\n",
      "[Epoch 3/3000] [Batch 5/137] [D loss: -49.102215] [G loss: -0.878766] [Total loss: -49.980981]\n",
      "[Epoch 3/3000] [Batch 10/137] [D loss: -47.206135] [G loss: -0.949220] [Total loss: -48.155355]\n",
      "[Epoch 3/3000] [Batch 15/137] [D loss: -49.702950] [G loss: -1.026150] [Total loss: -50.729100]\n",
      "[Epoch 3/3000] [Batch 20/137] [D loss: -49.643105] [G loss: -1.077694] [Total loss: -50.720799]\n",
      "[Epoch 3/3000] [Batch 25/137] [D loss: -50.309181] [G loss: -1.152473] [Total loss: -51.461654]\n",
      "[Epoch 3/3000] [Batch 30/137] [D loss: -55.574608] [G loss: -1.227880] [Total loss: -56.802488]\n",
      "[Epoch 3/3000] [Batch 35/137] [D loss: -55.971691] [G loss: -1.309621] [Total loss: -57.281312]\n",
      "[Epoch 3/3000] [Batch 40/137] [D loss: -53.235184] [G loss: -1.413104] [Total loss: -54.648288]\n",
      "[Epoch 3/3000] [Batch 45/137] [D loss: -57.149776] [G loss: -1.468125] [Total loss: -58.617901]\n",
      "[Epoch 3/3000] [Batch 50/137] [D loss: -58.882427] [G loss: -1.553316] [Total loss: -60.435743]\n",
      "[Epoch 3/3000] [Batch 55/137] [D loss: -58.866150] [G loss: -1.679727] [Total loss: -60.545877]\n",
      "[Epoch 3/3000] [Batch 60/137] [D loss: -59.402050] [G loss: -1.758091] [Total loss: -61.160141]\n",
      "[Epoch 3/3000] [Batch 65/137] [D loss: -64.135391] [G loss: -1.830450] [Total loss: -65.965841]\n",
      "[Epoch 3/3000] [Batch 70/137] [D loss: -60.277271] [G loss: -1.938456] [Total loss: -62.215727]\n",
      "[Epoch 3/3000] [Batch 75/137] [D loss: -66.113503] [G loss: -2.049090] [Total loss: -68.162593]\n",
      "[Epoch 3/3000] [Batch 80/137] [D loss: -66.138542] [G loss: -2.152425] [Total loss: -68.290967]\n",
      "[Epoch 3/3000] [Batch 85/137] [D loss: -68.237076] [G loss: -2.284288] [Total loss: -70.521364]\n",
      "[Epoch 3/3000] [Batch 90/137] [D loss: -68.463493] [G loss: -2.342071] [Total loss: -70.805564]\n",
      "[Epoch 3/3000] [Batch 95/137] [D loss: -65.344406] [G loss: -2.453761] [Total loss: -67.798167]\n",
      "[Epoch 3/3000] [Batch 100/137] [D loss: -72.463676] [G loss: -2.596368] [Total loss: -75.060044]\n",
      "[Epoch 3/3000] [Batch 105/137] [D loss: -67.382393] [G loss: -2.673373] [Total loss: -70.055766]\n",
      "[Epoch 3/3000] [Batch 110/137] [D loss: -70.658913] [G loss: -2.805907] [Total loss: -73.464820]\n",
      "[Epoch 3/3000] [Batch 115/137] [D loss: -76.607117] [G loss: -2.916537] [Total loss: -79.523654]\n",
      "[Epoch 3/3000] [Batch 120/137] [D loss: -80.886871] [G loss: -3.069441] [Total loss: -83.956312]\n",
      "[Epoch 3/3000] [Batch 125/137] [D loss: -76.298454] [G loss: -3.179475] [Total loss: -79.477929]\n",
      "[Epoch 3/3000] [Batch 130/137] [D loss: -77.175507] [G loss: -3.321650] [Total loss: -80.497157]\n",
      "[Epoch 3/3000] [Batch 135/137] [D loss: -78.941635] [G loss: -3.497910] [Total loss: -82.439545]\n",
      "[Epoch 4/3000] [Batch 0/137] [D loss: -77.139275] [G loss: -3.562952] [Total loss: -80.702227]\n",
      "[Epoch 4/3000] [Batch 5/137] [D loss: -81.280769] [G loss: -3.735327] [Total loss: -85.016096]\n",
      "[Epoch 4/3000] [Batch 10/137] [D loss: -80.874352] [G loss: -3.888973] [Total loss: -84.763325]\n",
      "[Epoch 4/3000] [Batch 15/137] [D loss: -84.209396] [G loss: -4.022526] [Total loss: -88.231922]\n",
      "[Epoch 4/3000] [Batch 20/137] [D loss: -88.651619] [G loss: -4.177136] [Total loss: -92.828755]\n",
      "[Epoch 4/3000] [Batch 25/137] [D loss: -92.488899] [G loss: -4.330151] [Total loss: -96.819050]\n",
      "[Epoch 4/3000] [Batch 30/137] [D loss: -90.528961] [G loss: -4.558895] [Total loss: -95.087856]\n",
      "[Epoch 4/3000] [Batch 35/137] [D loss: -93.835442] [G loss: -4.670954] [Total loss: -98.506395]\n",
      "[Epoch 4/3000] [Batch 40/137] [D loss: -99.862366] [G loss: -4.820843] [Total loss: -104.683208]\n",
      "[Epoch 4/3000] [Batch 45/137] [D loss: -97.777443] [G loss: -5.102378] [Total loss: -102.879821]\n",
      "[Epoch 4/3000] [Batch 50/137] [D loss: -98.077248] [G loss: -5.227586] [Total loss: -103.304833]\n",
      "[Epoch 4/3000] [Batch 55/137] [D loss: -100.741394] [G loss: -5.417918] [Total loss: -106.159312]\n",
      "[Epoch 4/3000] [Batch 60/137] [D loss: -100.114395] [G loss: -5.555667] [Total loss: -105.670062]\n",
      "[Epoch 4/3000] [Batch 65/137] [D loss: -97.423622] [G loss: -5.852685] [Total loss: -103.276307]\n",
      "[Epoch 4/3000] [Batch 70/137] [D loss: -108.170441] [G loss: -6.025607] [Total loss: -114.196048]\n",
      "[Epoch 4/3000] [Batch 75/137] [D loss: -105.443512] [G loss: -6.205173] [Total loss: -111.648685]\n",
      "[Epoch 4/3000] [Batch 80/137] [D loss: -109.696075] [G loss: -6.473202] [Total loss: -116.169277]\n",
      "[Epoch 4/3000] [Batch 85/137] [D loss: -107.896004] [G loss: -6.661924] [Total loss: -114.557928]\n",
      "[Epoch 4/3000] [Batch 90/137] [D loss: -109.058548] [G loss: -6.835407] [Total loss: -115.893955]\n",
      "[Epoch 4/3000] [Batch 95/137] [D loss: -114.370560] [G loss: -7.133951] [Total loss: -121.504511]\n",
      "[Epoch 4/3000] [Batch 100/137] [D loss: -113.569778] [G loss: -7.323200] [Total loss: -120.892978]\n",
      "[Epoch 4/3000] [Batch 105/137] [D loss: -115.858368] [G loss: -7.551303] [Total loss: -123.409671]\n",
      "[Epoch 4/3000] [Batch 110/137] [D loss: -117.127617] [G loss: -7.812855] [Total loss: -124.940472]\n",
      "[Epoch 4/3000] [Batch 115/137] [D loss: -126.531357] [G loss: -8.052725] [Total loss: -134.584082]\n",
      "[Epoch 4/3000] [Batch 120/137] [D loss: -118.150574] [G loss: -8.308563] [Total loss: -126.459137]\n",
      "[Epoch 4/3000] [Batch 125/137] [D loss: -113.878769] [G loss: -8.573851] [Total loss: -122.452620]\n",
      "[Epoch 4/3000] [Batch 130/137] [D loss: -123.028679] [G loss: -8.822920] [Total loss: -131.851599]\n",
      "[Epoch 4/3000] [Batch 135/137] [D loss: -120.933167] [G loss: -9.154890] [Total loss: -130.088057]\n",
      "[Epoch 5/3000] [Batch 0/137] [D loss: -125.055679] [G loss: -9.317795] [Total loss: -134.373474]\n"
     ]
    }
   ],
   "source": [
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "\n",
    "# if resuming training \n",
    "# generator.load_state_dict(torch.load(\"2024-11-05_14-41-32/50G.pth\", weights_only=True))\n",
    "# discriminator.load_state_dict(torch.load(\"2024-11-05_14-41-32/50D.pth\", weights_only=False))\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# scheduler_G = lr_scheduler.LinearLR(optimizer_G, start_factor=1.0, end_factor=0.3, total_iters=10)\n",
    "# scheduler_D = lr_scheduler.LinearLR(optimizer_D, start_factor=1.0, end_factor=0.3, total_iters=10)\n",
    "\n",
    "# might be possible to lower the precision but I chose to have more stability.\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "torch.set_float32_matmul_precision(\"highest\")\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "# start_epoch, loss_G, loss_D = load_checkpoint(\n",
    "#     '/kaggle/input/124/pytorch/default/1/checkpoint_epoch_650_20241231_042627.pth',\n",
    "#     generator,\n",
    "#     discriminator,\n",
    "#     optimizer_G,\n",
    "#     optimizer_D\n",
    "# )\n",
    "\n",
    "batches_done = 0\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for i, imgs in enumerate(dataloader):\n",
    "\n",
    "        # if epoch % 190 == 0:\n",
    "        #     torch.save(generator.state_dict(), f\"{epoch}\" + \"G.pth\")\n",
    "        #     torch.save(discriminator.state_dict(), f\"{epoch}\"+ \"D.pth\")\n",
    "\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        fake_imgs = generator(z).detach()\n",
    "        # Adversarial loss\n",
    "        loss_D = -torch.mean(discriminator(real_imgs)) + torch.mean(discriminator(fake_imgs))\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Clip weights of discriminator\n",
    "        for p in discriminator.parameters():\n",
    "            p.data.clamp_(-clip_value, clip_value)\n",
    "        # print(lr)\n",
    "        # scheduler_G.step()\n",
    "\n",
    "        # Train the generator every n_critic iterations\n",
    "        if i % n_critic == 0:\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generate a batch of images\n",
    "            gen_imgs = generator(z)\n",
    "            # Adversarial loss\n",
    "            loss_G = -torch.mean(discriminator(gen_imgs))\n",
    "\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # scheduler_D.step()\n",
    "\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] [Total loss: %f]\"\n",
    "                % (epoch, n_epochs, batches_done % len(dataloader), len(dataloader), loss_D.item(), loss_G.item(), loss_D.item() + loss_G.item())\n",
    "            )\n",
    "            \n",
    "\n",
    "        # if batches_done % sample_interval == 0:\n",
    "        #     # #save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "        #     # save_image(gen_imgs[:25], f\"{now_str}/{batches_done}.png\", nrow=5, normalize=True)\n",
    "        #     wandb.log({\"examples\": [wandb.Image(image) for image in gen_imgs[:15]]})\n",
    "\n",
    "        wandb.log({\"epoch\": epoch+650, \"loss_D\": loss_D, \"loss_G\": loss_G, \"total_loss\": loss_G + loss_D})\n",
    "        batches_done += 1\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        wandb.log({\"examples\": [wandb.Image(image) for image in gen_imgs[:10]]})\n",
    "    # Model saving\n",
    "    # if epoch % 50 == 0:\n",
    "    #     save_path = save_checkpoint(\n",
    "    #         generator,\n",
    "    #         discriminator,\n",
    "    #         optimizer_G,\n",
    "    #         optimizer_D,\n",
    "    #         epoch+650,\n",
    "    #         loss_G.item(),\n",
    "    #         loss_D.item(),\n",
    "    #         save_dir='/kaggle/working/checkpoints'\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
